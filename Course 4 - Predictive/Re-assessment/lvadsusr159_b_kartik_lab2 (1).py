# -*- coding: utf-8 -*-
"""lvadsusr159_b_kartik_lab2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NUI_T3Z6gESuuqu5YILTQ2a2f5ZrsoIx
"""

import pandas as pd
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.preprocessing import StandardScaler, LabelEncoder

from sklearn.model_selection import train_test_split

df= pd.read_csv('/content/mushroom.csv')

# Printing the DataFrame
df.head()

# Basic analysis of data
df.shape

df.info()

df.dtypes

df.isnull().sum()

# There are no categorical columns so there is no need of encoding

num_cols=df.select_dtypes(include='number').columns.tolist()

## Univariate analaysis
# Plotting histograms for numerical columns
for column in df.select_dtypes(include=['float64', 'int64']).columns:
    plt.figure(figsize=(10, 5))
    sns.histplot(df[column])
    plt.title(f'Histogram of {column}')
    plt.xlabel(column)
    plt.ylabel('Frequency')
    plt.show()

# Null values are less than 5% of the whole data set so we can remove them
df.dropna(inplace=True)

# Handling duplicate values
df.duplicated().sum()

df.drop_duplicates(inplace=True)

df.dtypes

## Outlier Detection

plt.figure(figsize = (20,5))
sns.boxplot(df)

# We can see that stem-width and cap-diameter has the most number of outliers

num_cols=df.select_dtypes(include='number').columns.tolist()

def remove_outliers(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]

for column in num_cols:
    df = remove_outliers(df, column)

## FEATURE SELECTION
## Correlation Analysis
numerical_columns = df.select_dtypes(include='number').columns.tolist()

plt.figure(figsize=(10, 6))
sns.heatmap(df[numerical_columns].corr(), annot=True)
plt.title('Heatmap of Correlation Matrix')
plt.show()

# Based on the above heatmap we have done feature selection

target=df['class']

# Separating the dataset into feature variables (X) and the response variable (y)
X = df.drop(['class'], axis=1)
y = target

# scaling
ss=StandardScaler()
X=ss.fit_transform(X)

# Splitting the resampled data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Using RandomForestClassifier on the resampled data
rfc = RandomForestClassifier()
rfc.fit(X_train, y_train)

# Predictions
pred_rfc = rfc.predict(X_test)

# Model performance
print("\nClassification Report:")
print(classification_report(y_test, pred_rfc))

# Confusion matrix
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, pred_rfc))

cm = confusion_matrix(y_test, pred_rfc)

# Plotting the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=rfc.classes_, yticklabels=rfc.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# We can conclude that RandomForestClassifier is the performing very well for this dataset.