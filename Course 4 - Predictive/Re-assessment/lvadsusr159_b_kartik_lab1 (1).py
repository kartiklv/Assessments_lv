# -*- coding: utf-8 -*-
"""lvadsusr159_b_kartik_lab1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HYkqu3KkADcD0gGD9y2euM7fvcOMxZcx
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PolynomialFeatures
from sklearn.preprocessing import StandardScaler, LabelEncoder

from sklearn.linear_model import LinearRegression

from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error

### Loading the data
df = pd.read_csv('/content/bengaluru_house_prices.csv')
df.head()

# Basic analysis of data
df.shape

df.info()

df.dtypes

df['society'].value_counts()

df['location'].value_counts()

df.drop(['society'],axis=1,inplace=True)

df.isnull().sum()

num_cols=df.select_dtypes(include='number').columns.tolist()
cat_cols=df.select_dtypes(exclude='number').columns.tolist()

## Univariate analaysis
# Plotting histograms for numerical columns
for column in num_cols:
    plt.figure(figsize=(10, 5))
    sns.histplot(df[column])
    plt.title(f'Histogram of {column}')
    plt.xlabel(column)
    plt.ylabel('Frequency')
    plt.show()

# By analysing the data we are handling null values
df.dropna(inplace=True)
df.isnull().sum()

df.duplicated().sum()

# Handling duplicates
df.drop_duplicates(inplace=True)

# Plotting bar charts for analysing categorical columns
for column in cat_cols:
    plt.figure(figsize=(10, 5))
    df[column].value_counts().plot(kind='bar')
    plt.title(f'Bar Chart of {column}')
    plt.xlabel(column)
    plt.ylabel('Count')
    plt.show()

df.columns

# Outlier Detection
num_cols=df.select_dtypes(include='number').columns.tolist()
for column in num_cols:
    plt.figure(figsize=(10, 6))
    sns.boxplot(x=df[column])
    plt.title(f'Box Plot of {column}')
    plt.xlabel(column)
    plt.show()

#  Bath and size has the most number of outliers

df.columns

num_cols=df.select_dtypes(include='number').columns.tolist()


def remove_outliers(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]

for column in num_cols:
    df = remove_outliers(df, column)

## FEATURE SELECTION
## Correlation Analysis
numerical_columns = df.select_dtypes(include='number').columns.tolist()

plt.figure(figsize=(10, 6))
sns.heatmap(df[numerical_columns].corr(), annot=True)
plt.title('Heatmap of Correlation Matrix')
plt.show()

## Encoding categorical columns
label_quality = LabelEncoder()
for column in cat_cols:
  df[column] = label_quality.fit_transform(df[column])

# from the analysis we have selected the columns
target=df['price']

# Separate the dataset into feature variables (X) and the response variable (y)
X = df.drop(['price'], axis=1)
y = target

# scaling
ss=StandardScaler()
X=ss.fit_transform(X)

# Splitting data into train and test

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = LinearRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

# Evaluation of the model
mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)


print("Mean Squared Error:", mse)
print("Mean Absolute Error:", mae)
print("Root Mean Squared Error:", rmse)
print("R-squared Score:", r2)

# Best fit line
plt.figure(figsize=(8, 6))
plt.scatter(y_test, y_pred, color='blue', alpha=0.5, label='Actual vs. Predicted')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--', label='Ideal line')
plt.xlabel('Actual')
plt.ylabel('Predicted')
plt.title('Actual vs. Predicted')
plt.legend()
plt.show()

# After evaluating our model we can say that Linear regression is giving us good results.