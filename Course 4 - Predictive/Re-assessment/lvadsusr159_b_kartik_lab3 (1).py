# -*- coding: utf-8 -*-
"""lvadsusr159_b_kartik_lab3

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nNCfCIlAp4gJ5nI4oE_2IqqfALAmbtrj
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.preprocessing import MinMaxScaler, LabelEncoder

### Loading the data
df=pd.read_csv("/content/Credit Card Customer Data.csv")
df.head()

# Basic analysis of data
df.info()

df.dtypes

# There are no categorical columns so there is no need of encoding

df.columns

# we can see that these columns are unnecessary for our clustering model
df.drop(['Sl_No','Customer Key'],axis=1,inplace=True)

df.shape

df.describe()

df.isnull().sum()

## Univariate analaysis
# Plotting histograms for Total_visits_online
plt.figure(figsize=(10, 5))
sns.histplot(df['Total_visits_online'])
plt.title(f'Histogram of Total_visits_online')
plt.xlabel('Total_visits_online')
plt.ylabel('Frequency')
plt.show()

# Handling null values
df.fillna(df.median(), inplace=True)
df.isnull().sum()

df.duplicated().sum()

# Handling duplicates
df.drop_duplicates(inplace=True)

## Outlier Detection
plt.figure(figsize = (20,5))
sns.boxplot(df)

# We can see that the most number of outliers are in Avg_Credit_Limit

# Handling outliers
num_cols=df.select_dtypes(include='number').columns.tolist()

def remove_outliers(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]

for column in num_cols:
    df = remove_outliers(df, column)

## Bivariate Analysis
# Plotting scatter plots for pairs of numerical columns
numerical_columns = df.select_dtypes(include=['float64', 'int64']).columns
for i in range(len(numerical_columns)):
    for j in range(i + 1, len(numerical_columns)):
        plt.figure(figsize=(10, 5))
        sns.scatterplot(x=df[numerical_columns[i]], y=df[numerical_columns[j]])
        plt.title(f'Scatter Plot of {numerical_columns[i]} vs {numerical_columns[j]}')
        plt.xlabel(numerical_columns[i])
        plt.ylabel(numerical_columns[j])
        plt.show()

## Correlation Analysis
numerical_columns = df.select_dtypes(include='number').columns.tolist()

plt.figure(figsize=(10, 6))
sns.heatmap(df[numerical_columns].corr(), annot=True)
plt.title('Heatmap of Correlation Matrix')
plt.show()

df.columns

X=df.drop(columns="Avg_Credit_Limit")
y=df["Avg_Credit_Limit"]

mi_ma_s = MinMaxScaler()
X = mi_ma_s.fit_transform(X)

## Model implementation using k-means
from sklearn.cluster import KMeans
cs = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters = i, max_iter = 300, n_init = 10, random_state = 0)
    kmeans.fit(X)
    cs.append(kmeans.inertia_)

plt.plot(range(1, 11), cs)
plt.title('The Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('CS')
plt.show()

from sklearn.preprocessing import StandardScaler

X = df[['Avg_Credit_Limit', 'Total_visits_online']].values
scaler = StandardScaler()
X= scaler.fit_transform(X)

# From the elbow curve we can see the optimal number of clusters(i.e 2).

k = 2

kmeans = KMeans(n_clusters=k)
pred=kmeans.fit_predict(X)

df['Cluster'] = kmeans.labels_

centroids = scaler.inverse_transform(kmeans.cluster_centers_)

cluster_df={}
plt.figure(figsize=(8, 6))

for i in range(k):
    cluster_data = df[df['Cluster'] == i]
    cluster_df["".join(["df",str(i)])]=cluster_data
    plt.scatter(cluster_data['Avg_Credit_Limit'], cluster_data['Total_visits_online'],
                label=f'Cluster {i}')

plt.scatter(centroids[:, 0], centroids[:, 1], marker='*', s=300, c='red', label='Centroids')

plt.grid(True)
plt.show()

from sklearn.preprocessing import StandardScaler

X = df[['Avg_Credit_Limit', 'Total_visits_online']].values
scaler = StandardScaler()
X= scaler.fit_transform(X)

k = 2

kmeans = KMeans(n_clusters=k)
pred=kmeans.fit_predict(X)

df['Cluster'] = kmeans.labels_

centroids = scaler.inverse_transform(kmeans.cluster_centers_)

cluster_df.clear()
plt.figure(figsize=(8, 6))

for i in range(k):
    cluster_data = df[df['Cluster'] == i]
    cluster_df["".join(["df",str(i)])]=cluster_data
    plt.scatter(cluster_data['Avg_Credit_Limit'], cluster_data['Total_visits_online'],
                label=f'Cluster {i}')

plt.scatter(centroids[:, 0], centroids[:, 1], marker='*', s=300, c='red', label='Centroids')

plt.grid(True)
plt.show()

print("Silhouette score: ",silhouette_score(df[['Avg_Credit_Limit','Total_visits_online']], kmeans.fit_predict(df[['Avg_Credit_Limit','Total_visits_online']])))

### Cluster Analysis
print(cluster_df['df0'][['Avg_Credit_Limit']].mean())
print(cluster_df['df1'][['Avg_Credit_Limit']].mean())

## We can segement customers as follows:

# Cluster 0 represents low spenders
# Cluster 1 represents high spenders

### Business Recomendation

# Considering that low spenders are very cost conscious, we can provide them discounts to encourage them to spend more from their credit cards.
# For the high spenders, we can create campaigns for limited edition items which will take advantage of their high spending habits.